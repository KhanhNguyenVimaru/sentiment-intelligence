Research: Tối ưu tốc độ phản hồi cho EmoGuest
------------------------------------------------

Mục tiêu: giảm độ trễ phản hồi từ mô hình Ollama trong kiến trúc hiện tại (Vue + FastAPI + Ollama).

1) Tối ưu prompt và tham số sinh
- Rút gọn prompt: giữ đúng yêu cầu nhãn + format JSON, bỏ lời thoại thừa để mô hình xử lý nhanh hơn.
- Giảm num_predict: từ 128 xuống 16–32 vì output chỉ vài token. Kỳ vọng giảm thời gian sinh token đầu tiên và tổng thời gian hoàn thành.
- Giữ temperature = 0 để kết quả ổn định và giảm độ phân nhánh.

2) Bật streaming và hiển thị sớm
- Gửi request với stream=true tới Ollama; backend forward chunk đầu tiên cho frontend để người dùng thấy kết quả sớm.
- Vue nhận stream (ReadableStream) và hiển thị khi đã đọc đủ JSON hợp lệ, không cần đợi toàn bộ completion.
- Lưu ý: kết nối HTTP cần giữ mở tới khi xong, nên dùng Fetch stream trên frontend và yield dần trên backend.

3) Giữ nóng mô hình
- Sau khi khởi động backend, gửi 1 yêu cầu warm-up (ví dụ câu ngắn) để Ollama tải model vào RAM/VRAM, tránh latency đợt gọi đầu.
- Có thể đặt /health nâng cao: kiểm tra và tự gọi warm-up nếu chưa có cache model.

4) Kiến trúc và luồng hiện tại (simplified)
- Frontend (Vue): textarea nhập câu -> gọi POST http://localhost:8000/classify -> render trạng thái loading + kết quả.
- Backend (FastAPI, backend/api_server.py): endpoint /classify nhận sentence -> build prompt -> POST tới Ollama API -> trả predicted_emotion + done_reason.
- Ollama: model gpt-oss:20b chạy local, trả completion JSON (hoặc text).

5) Đề xuất chỉnh code (action items)
- backend/api_server.py:
  * prompt: ngắn gọn (nhãn allowed + yêu cầu JSON).
  * num_predict: 16–32.
  * stream: True; xử lý streaming response và trả về sớm cho client (có thể chuyển endpoint sang StreamingResponse).
  * Tạo requests.Session để reuse TCP connection.
  * Thêm warm-up hàm gọi classify("warmup") khi app khởi động.
- frontend App.vue:
  * Hỗ trợ đọc stream (fetch with ReadableStream) và update UI ngay khi parse được JSON.
  * Giữ progress indicator trong lúc stream đang tới.

6) Lợi ích kỳ vọng
- Giảm TTFB: rút gọn prompt + num_predict + warm-up.
- Giảm tổng latency: stream + ít token sinh.
- Trải nghiệm người dùng: thấy kết quả sớm hơn nhờ stream + loading rõ ràng.

7) Rủi ro / kiểm thử
- Prompt quá ngắn có thể làm giảm độ chính xác; cần A/B với dataset nhỏ.
- Stream yêu cầu client/ server xử lý chunk; kiểm thử trên trình duyệt hỗ trợ fetch streaming.
- Warm-up tăng chi phí khởi động nhưng giảm spike request đầu.
